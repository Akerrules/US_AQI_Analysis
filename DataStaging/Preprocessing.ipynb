{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mXS_D7HFQ0Z"
      },
      "source": [
        "This Python notebook is programmed to perform the ETL process of the data staging of the project. Each code bloc is guided by a text section to explain the purpose, functionality and necessity with respect to the project progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u_dQsnfGJiZ"
      },
      "source": [
        "**Importing of Libraries**\n",
        "\n",
        "- `pandas`: Using the `dataframe` data structure and functions handle, manipulate and analyze large datasets.\n",
        "\n",
        "- `os`: to interact with the operating system, such as creating directories, reading and writing files, and changing the current working directory.\n",
        "\n",
        "- `googlemaps`: Using Google Maps API to access geospatial data and services, and in particular to find locations of certain places."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSK36hwzI6Ht",
        "outputId": "0d1a2f22-484b-4bce-fbcf-f86400d07545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googlemaps in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.10.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from googlemaps) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\injus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U googlemaps\n",
        "import pandas as pd\n",
        "import os\n",
        "import googlemaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_9HQRdDrvfSV"
      },
      "outputs": [],
      "source": [
        "# Need to assign a unique API key, which authenticates requests associated with project to use Google products\n",
        "# API key is stored in an environment variable --> removed for security purposes\n",
        "gmaps = googlemaps.Client(key='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb6B1bzuw4tk"
      },
      "source": [
        "Read all the value from the weather data folder and combined all the weather csv into one dataframe. This allows to consolidate the multiple `.csv` files into a single DataFrame for easier analysis and manipulation, as the data is prepared for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "S8AG7puVwcLC",
        "outputId": "47cb9503-e921-4e72-e0a9-c045adcedc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total files 210\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>prcp</th>\n",
              "      <th>station_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1894-01-01</td>\n",
              "      <td>60.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USC00042863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1894-01-02</td>\n",
              "      <td>58.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>USC00042863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1894-01-03</td>\n",
              "      <td>57.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USC00042863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1894-01-04</td>\n",
              "      <td>53.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.28</td>\n",
              "      <td>USC00042863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1894-01-05</td>\n",
              "      <td>50.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USC00042863</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  tmax  tmin  prcp   station_id\n",
              "0  1894-01-01  60.0  41.0  0.00  USC00042863\n",
              "1  1894-01-02  58.0  50.0  0.40  USC00042863\n",
              "2  1894-01-03  57.0  42.0  0.00  USC00042863\n",
              "3  1894-01-04  53.0  42.0  0.28  USC00042863\n",
              "4  1894-01-05  50.0  38.0  0.00  USC00042863"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set the path to the directory where the data files are stored\n",
        "folder_path = './Weather_data'\n",
        "\n",
        "# List all files and directories in the above folder\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# create a list of files that start with \"US\"\n",
        "us_files = [file for file in files if file.startswith(\"US\")]\n",
        "print(\"total files\", len(us_files))\n",
        "# Read each CSV into a DataFrame and store them in a list\n",
        "\n",
        "dataframes = [] # Empty list to store DataFrames\n",
        "for file in us_files:\n",
        "    df = pd.read_csv(f\"./Weather_data/{file}\")\n",
        "    # Extract station name from filename (remove '.csv' part)\n",
        "    station_name = file[:-4]  # This removes the last 4 characters, assuming they are '.csv'\n",
        "    # Add a new column with the station name\n",
        "    df['station_id'] = station_name\n",
        "    dataframes.append(df)\n",
        "# Concatenate all DataFrames in the list into one\n",
        "weather_df = pd.concat(dataframes, ignore_index=True)\n",
        "weather_df  =  weather_df.drop(columns=weather_df.columns[0], axis=1) # drop the first column)\n",
        "weather_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nswqE4lOw4tr"
      },
      "source": [
        "This code bloc is used for data exploration and checking of data quality, specifically by assessing the amount of missing data. As a part of data preprocessing, this is to help inform decisions about how to handle these missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7wntankn7q1",
        "outputId": "5310ac28-1a89-4a08-eb85-6c6c31ea33a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date           object\n",
            "tmax          float64\n",
            "tmin          float64\n",
            "prcp          float64\n",
            "station_id     object\n",
            "dtype: object\n",
            "tmax 583903 10475914\n",
            "tmax 5.573766642223294\n",
            "tmin 583010 10475914\n",
            "tmin 5.565242326349758\n",
            "prcp 525810 10475914\n",
            "prcp 5.019227916533106\n",
            "total NaN 6.861959729719048\n"
          ]
        }
      ],
      "source": [
        "# Print out data types for weather_df and check how many many null value there are in each column\n",
        "print(weather_df.dtypes)\n",
        "print(\"tmax\",weather_df['tmax'].isna().sum(),weather_df.shape[0] )\n",
        "print(\"tmax\", (weather_df['tmax'].isna().sum())/weather_df.shape[0] *100)\n",
        "print(\"tmin\",weather_df['tmin'].isna().sum(),weather_df.shape[0] )\n",
        "print(\"tmin\", (weather_df['tmin'].isna().sum())/weather_df.shape[0] *100)\n",
        "print(\"prcp\",weather_df['prcp'].isna().sum(),weather_df.shape[0] )\n",
        "print(\"prcp\", (weather_df['prcp'].isna().sum())/weather_df.shape[0] *100)\n",
        "print(\"total NaN\",  weather_df.isna().any(axis=1).sum()/ weather_df.shape[0]*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo9haKSZw4tt"
      },
      "source": [
        "Given the results acquired by the previous step, most of the missing values are around 5% of the total available data. Hence, with respect to the size of the data repositroy, the missing (null) values can be simply droppped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ali5h6Fdw4tt"
      },
      "outputs": [],
      "source": [
        "# drop all the NaN Values\n",
        "weather_df = weather_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4pbjwFUw4tt",
        "outputId": "aca22230-6b27-4c01-ddf0-38c9e687b6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total NaN 0\n",
            "            Date  tmax  tmin  prcp   station_id\n",
            "1096  1874-01-01  60.0  36.0  0.00  USW00003822\n",
            "1097  1874-01-02  62.0  43.0  0.02  USW00003822\n",
            "1098  1874-01-03  66.0  53.0  0.12  USW00003822\n",
            "1099  1874-01-04  70.0  59.0  0.66  USW00003822\n",
            "1100  1874-01-05  66.0  58.0  0.50  USW00003822\n"
          ]
        }
      ],
      "source": [
        "# check if all the values are dropped\n",
        "print(\"total NaN\",  weather_df.isna().any(axis=1).sum())\n",
        "print(weather_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iq3G7_Uw4tu"
      },
      "source": [
        "As correct data types and formats are essential for accurate and efficient data analysis, this code bloc converts the data to the right types and format, in this case on the `weather_df` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3prShk8KoLma",
        "outputId": "d819b583-1ca2-4405-a636-f4863b94d94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Date  tmax  tmin  prcp   station_id\n",
            "0  1874-01-01    60    36  0.00  USW00003822\n",
            "1  1874-01-02    62    43  0.02  USW00003822\n",
            "2  1874-01-03    66    53  0.12  USW00003822\n",
            "3  1874-01-04    70    59  0.66  USW00003822\n",
            "4  1874-01-05    66    58  0.50  USW00003822\n",
            "5  1874-01-06    71    60  0.22  USW00003822\n",
            "6  1874-01-07    59    40  0.02  USW00003822\n",
            "7  1874-01-08    51    35  0.00  USW00003822\n",
            "8  1874-01-09    59    35  0.03  USW00003822\n",
            "9  1874-01-10    61    45  0.02  USW00003822\n",
            "10 1874-01-11    68    46  0.00  USW00003822\n",
            "11 1874-01-12    56    46  0.00  USW00003822\n",
            "12 1874-01-13    54    46  0.00  USW00003822\n",
            "13 1874-01-14    54    45  0.05  USW00003822\n",
            "14 1874-01-15    46    34  0.00  USW00003822\n",
            "15 1874-01-16    41    31  0.00  USW00003822\n",
            "16 1874-01-17    49    29  0.00  USW00003822\n",
            "17 1874-01-18    61    38  0.01  USW00003822\n",
            "18 1874-01-19    63    43  0.00  USW00003822\n",
            "19 1874-01-20    71    45  0.03  USW00003822\n",
            "20 1874-01-21    64    51  0.09  USW00003822\n",
            "21 1874-01-22    72    55  0.10  USW00003822\n",
            "22 1874-01-23    73    59  0.02  USW00003822\n",
            "23 1874-01-24    69    46  0.00  USW00003822\n",
            "24 1874-01-25    53    36  0.00  USW00003822\n",
            "Date          datetime64[ns]\n",
            "tmax                   int64\n",
            "tmin                   int64\n",
            "prcp                 float64\n",
            "station_id            object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
        "weather_df[\"tmax\"] = weather_df['tmax'].astype(int)\n",
        "weather_df[\"tmin\"] = weather_df['tmin'].astype(int)\n",
        "weather_df = weather_df.reset_index()\n",
        "weather_df = weather_df.drop(\"index\",axis=1)\n",
        "# weather_df = weather_df.drop(weather_df.columns[0], axis=1)\n",
        "print(weather_df[:25])\n",
        "print(weather_df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq7TMu68w4tv"
      },
      "source": [
        "Generating surrogate keys for the `weather_df` dataframe to later on be used for indexing, joining data, and other data manipulation tasks, in contrast to the usage of primary (natural) keys which are not suitable in database management and data warehousing.\n",
        "This step is performed for each dataframe to be created, which to then be merged all together in accordance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKiBOOLSw4tv",
        "outputId": "9501f38c-1ced-492f-dc81-927bf527d822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Surrogate Keys       Date  tmax  tmin  prcp   station_id\n",
            "0               1 1874-01-01    60    36  0.00  USW00003822\n",
            "1               2 1874-01-02    62    43  0.02  USW00003822\n",
            "2               3 1874-01-03    66    53  0.12  USW00003822\n",
            "3               4 1874-01-04    70    59  0.66  USW00003822\n",
            "4               5 1874-01-05    66    58  0.50  USW00003822\n"
          ]
        }
      ],
      "source": [
        "# Generating surrogate keys for the weather dataframe and moving the column to the first column of the dataframe\n",
        "weather_df['Surrogate Keys'] = range(1,len(weather_df)+1)\n",
        "weather_df = weather_df.reindex(columns=['Surrogate Keys'] + list([c for c in weather_df.columns if c!= 'Surrogate Keys']))\n",
        "# print(weather_df.reset_index())\n",
        "weather_df = weather_df.reset_index()\n",
        "weather_df = weather_df.drop(\"index\",axis=1)\n",
        "print(weather_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYDJcg3-sgal"
      },
      "outputs": [],
      "source": [
        "# The method takes in the month of the given date value as an argument and returns the season\n",
        "def get_season(month):\n",
        "    if month in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    elif month in [9, 10, 11]:\n",
        "        return 'Autumn'\n",
        "    else: # month in [12, 1, 2]\n",
        "        return 'Winter'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the previous method, the `date_df` dataframe is to be created. In particular, it is to extract useful date-related features from the `Date` column. The new features can provide valuable insights and can be used in time series analysis, trend analysis, and predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "i9OO6BbroqjZ",
        "outputId": "1f464c38-e99e-4fb8-cd9b-2799e69d7224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>season</th>\n",
              "      <th>day_of_week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    year  month  day  season day_of_week\n",
              "0   1874      1    1  Winter    Thursday\n",
              "1   1874      1    2  Winter      Friday\n",
              "2   1874      1    3  Winter    Saturday\n",
              "3   1874      1    4  Winter      Sunday\n",
              "4   1874      1    5  Winter      Monday\n",
              "5   1874      1    6  Winter     Tuesday\n",
              "6   1874      1    7  Winter   Wednesday\n",
              "7   1874      1    8  Winter    Thursday\n",
              "8   1874      1    9  Winter      Friday\n",
              "9   1874      1   10  Winter    Saturday\n",
              "10  1874      1   11  Winter      Sunday\n",
              "11  1874      1   12  Winter      Monday\n",
              "12  1874      1   13  Winter     Tuesday\n",
              "13  1874      1   14  Winter   Wednesday\n",
              "14  1874      1   15  Winter    Thursday\n",
              "15  1874      1   16  Winter      Friday\n",
              "16  1874      1   17  Winter    Saturday\n",
              "17  1874      1   18  Winter      Sunday\n",
              "18  1874      1   19  Winter      Monday\n",
              "19  1874      1   20  Winter     Tuesday\n",
              "20  1874      1   21  Winter   Wednesday\n",
              "21  1874      1   22  Winter    Thursday\n",
              "22  1874      1   23  Winter      Friday\n",
              "23  1874      1   24  Winter    Saturday\n",
              "24  1874      1   25  Winter      Sunday"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the date column to datetime\n",
        "date_df = pd.DataFrame()\n",
        "print()\n",
        "# Extract year, month, and day\n",
        "date_df['year'] = weather_df['Date'].dt.year\n",
        "date_df['month'] = weather_df['Date'].dt.month\n",
        "date_df['day'] = weather_df['Date'].dt.day\n",
        "# date_df['season'] = getSeason(date_df['month'])\n",
        "date_df['season'] = date_df['month'].apply(get_season)\n",
        "\n",
        "# Combine year, month, and day to create a new date column\n",
        "date_df['date'] = pd.to_datetime(date_df[['year', 'month', 'day']])\n",
        "\n",
        "# Calculate the day of the week and assign it to a new column\n",
        "date_df['day_of_week'] = date_df['date'].dt.day_name()\n",
        "date_df = date_df\n",
        "date_df = date_df.drop(\"date\", axis=1)\n",
        "date_df[:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfwEnYJdw4tw",
        "outputId": "2a81c565-5c8b-4c6c-ab44-535e597d0d94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "year            int32\n",
              "month           int32\n",
              "day             int32\n",
              "season         object\n",
              "day_of_week    object\n",
              "dtype: object"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "date_df.dtypes # check the data types of the columns in the date_df dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "l7h9ovTXq5UY",
        "outputId": "47f86b3f-746a-4d28-b1bc-b9a1c092b487"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>season</th>\n",
              "      <th>day_of_week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys  year  month  day  season day_of_week\n",
              "0               1  1874      1    1  Winter    Thursday\n",
              "1               2  1874      1    2  Winter      Friday\n",
              "2               3  1874      1    3  Winter    Saturday\n",
              "3               4  1874      1    4  Winter      Sunday\n",
              "4               5  1874      1    5  Winter      Monday"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generating surrogate keys for the date dataframe and moving the column to the first column of the dataframe\n",
        "date_df['Surrogate Keys'] = range(1,len(date_df)+1)\n",
        "date_df = date_df.reindex(columns=['Surrogate Keys'] + list([c for c in date_df.columns if c!= 'Surrogate Keys']))\n",
        "date_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving onto the `station_df` dataframe, created to analyse the air quality data at the individual monitoring station level. The result of a cleaned and processed DataFrame containing station information(ID, city name, latitude, longitude, and status) to then be joined with other data based on the station ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJkx7IpJttft",
        "outputId": "3e419060-e49f-46dc-e431-b30c208ea046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "210\n",
            "        city_name           ID      Lat       Lon  Status\n",
            "0       Escondido  USC00042863  33.1211 -117.0900  Active\n",
            "1    Natchitoches  USC00166584  31.8142  -93.0856  Active\n",
            "2       Belvidere  USC00280734  40.8292  -75.0833  Active\n",
            "3    NewBrunswick  USC00286055  40.4728  -74.4225  Active\n",
            "4        Portland  USC00356749  45.5181 -122.6894  Active\n",
            "..            ...          ...      ...       ...     ...\n",
            "205      Waterloo  USW00094910  42.5544  -92.4011  Active\n",
            "206       Ashland  USW00094929  46.5486  -90.9189  Active\n",
            "207      Hastings  USW00094949  40.6044  -98.4272  Active\n",
            "208      Mitchell  USW00094950  43.7667  -98.0333  Active\n",
            "209       Hayward  USW00094973  46.0261  -91.4442  Active\n",
            "\n",
            "[210 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# create a station dataframe and add data from city.csv\n",
        "city_df = pd.read_csv(\"./Weather_data/city_info.csv\")\n",
        "# print(city_df, city_df.shape[0])\n",
        "\n",
        "print(len(weather_df['station_id'].unique())) # the number of unique station IDs in the weather_df DataFrame\n",
        "# Formatting steps: conversion to datetime format, sorting, and renaming columns\n",
        "city_df['Stn.stDate'] = pd.to_datetime(city_df['Stn.stDate'])\n",
        "city_df['Stn.edDate'] = pd.to_datetime(city_df['Stn.edDate'])\n",
        "city_df['city_name']= city_df['Name']\n",
        "city_df = city_df.drop('Name', axis=1)\n",
        "# print(city_df)\n",
        "# Sort DataFrame by ID and end date in ascending order\n",
        "city_df = city_df.sort_values(by=['ID', 'Stn.edDate'], ascending=[True, False])\n",
        "\n",
        "# Initialize an empty list to store IDs that have been marked as active\n",
        "latest_status = {}\n",
        "city_df['Status'] = None\n",
        "\n",
        "city_df['Status'] = city_df['Status'].astype(str)\n",
        "# Iterate through the DataFrame to update status for each ID\n",
        "for index, row in city_df.iterrows():\n",
        "    if row['ID'] not in latest_status:\n",
        "        latest_status[row['ID']] = 'Active' if row['Stn.edDate'] == pd.to_datetime(\"2021-12-31\") else 'Inactive'\n",
        "    city_df.loc[index, 'Status'] = latest_status[row['ID']]\n",
        "\n",
        "# Add 'Status' column to DataFrame\n",
        "# df['Status'] = statuses\n",
        "    # removes duplicate rows based on 'ID' and drops any remaining rows with missing values\n",
        "    city_df = city_df.drop_duplicates(subset=['ID']).dropna()\n",
        "\n",
        "# Select only necessary columns\n",
        "result_df = city_df[['city_name', 'ID','Lat','Lon','Status']].reset_index()\n",
        "\n",
        "station_df = result_df.drop(\"index\", axis=1)\n",
        "print(station_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFT-Qryaw4ty",
        "outputId": "1b398f59-9cff-4a63-a4fe-65be8386b4f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>city_name</th>\n",
              "      <th>ID</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Lon</th>\n",
              "      <th>Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Escondido</td>\n",
              "      <td>USC00042863</td>\n",
              "      <td>33.1211</td>\n",
              "      <td>-117.0900</td>\n",
              "      <td>Active</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Natchitoches</td>\n",
              "      <td>USC00166584</td>\n",
              "      <td>31.8142</td>\n",
              "      <td>-93.0856</td>\n",
              "      <td>Active</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Belvidere</td>\n",
              "      <td>USC00280734</td>\n",
              "      <td>40.8292</td>\n",
              "      <td>-75.0833</td>\n",
              "      <td>Active</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>NewBrunswick</td>\n",
              "      <td>USC00286055</td>\n",
              "      <td>40.4728</td>\n",
              "      <td>-74.4225</td>\n",
              "      <td>Active</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Portland</td>\n",
              "      <td>USC00356749</td>\n",
              "      <td>45.5181</td>\n",
              "      <td>-122.6894</td>\n",
              "      <td>Active</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys     city_name           ID      Lat       Lon  Status\n",
              "0               1     Escondido  USC00042863  33.1211 -117.0900  Active\n",
              "1               2  Natchitoches  USC00166584  31.8142  -93.0856  Active\n",
              "2               3     Belvidere  USC00280734  40.8292  -75.0833  Active\n",
              "3               4  NewBrunswick  USC00286055  40.4728  -74.4225  Active\n",
              "4               5      Portland  USC00356749  45.5181 -122.6894  Active"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Generating surrogate keys for the station dataframe and moving the column to the first column of the dataframe\n",
        "station_df['Surrogate Keys'] = range(1,len(station_df)+1)\n",
        "station_df = station_df.reindex(columns=['Surrogate Keys'] + list([c for c in station_df.columns if c!= 'Surrogate Keys']))\n",
        "station_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `US_AQI.csv` dataset is the main dataset, containing the measures of the FACT table to be created at the end of the ETL process. In addition, some new features are engineered to enrich the data analysis aspects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BewG0ZH0w4ty",
        "outputId": "fe8eae62-c5dd-4203-f42a-2ad652551033"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>CBSA Code</th>\n",
              "      <th>Date</th>\n",
              "      <th>AQI</th>\n",
              "      <th>Category</th>\n",
              "      <th>Defining Parameter</th>\n",
              "      <th>Number of Sites Reporting</th>\n",
              "      <th>city_ascii</th>\n",
              "      <th>state_id</th>\n",
              "      <th>state_name</th>\n",
              "      <th>lat</th>\n",
              "      <th>lng</th>\n",
              "      <th>population</th>\n",
              "      <th>density</th>\n",
              "      <th>timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10140</td>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>21</td>\n",
              "      <td>Good</td>\n",
              "      <td>PM2.5</td>\n",
              "      <td>2</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>WA</td>\n",
              "      <td>Washington</td>\n",
              "      <td>46.9757</td>\n",
              "      <td>-123.8094</td>\n",
              "      <td>16571.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10140</td>\n",
              "      <td>2022-01-02</td>\n",
              "      <td>12</td>\n",
              "      <td>Good</td>\n",
              "      <td>PM2.5</td>\n",
              "      <td>2</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>WA</td>\n",
              "      <td>Washington</td>\n",
              "      <td>46.9757</td>\n",
              "      <td>-123.8094</td>\n",
              "      <td>16571.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10140</td>\n",
              "      <td>2022-01-03</td>\n",
              "      <td>18</td>\n",
              "      <td>Good</td>\n",
              "      <td>PM2.5</td>\n",
              "      <td>2</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>WA</td>\n",
              "      <td>Washington</td>\n",
              "      <td>46.9757</td>\n",
              "      <td>-123.8094</td>\n",
              "      <td>16571.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10140</td>\n",
              "      <td>2022-01-04</td>\n",
              "      <td>19</td>\n",
              "      <td>Good</td>\n",
              "      <td>PM2.5</td>\n",
              "      <td>2</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>WA</td>\n",
              "      <td>Washington</td>\n",
              "      <td>46.9757</td>\n",
              "      <td>-123.8094</td>\n",
              "      <td>16571.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10140</td>\n",
              "      <td>2022-01-05</td>\n",
              "      <td>17</td>\n",
              "      <td>Good</td>\n",
              "      <td>PM2.5</td>\n",
              "      <td>2</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>WA</td>\n",
              "      <td>Washington</td>\n",
              "      <td>46.9757</td>\n",
              "      <td>-123.8094</td>\n",
              "      <td>16571.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  CBSA Code        Date  AQI Category Defining Parameter  \\\n",
              "0           0      10140  2022-01-01   21     Good              PM2.5   \n",
              "1           1      10140  2022-01-02   12     Good              PM2.5   \n",
              "2           2      10140  2022-01-03   18     Good              PM2.5   \n",
              "3           3      10140  2022-01-04   19     Good              PM2.5   \n",
              "4           4      10140  2022-01-05   17     Good              PM2.5   \n",
              "\n",
              "   Number of Sites Reporting city_ascii state_id  state_name      lat  \\\n",
              "0                          2   Aberdeen       WA  Washington  46.9757   \n",
              "1                          2   Aberdeen       WA  Washington  46.9757   \n",
              "2                          2   Aberdeen       WA  Washington  46.9757   \n",
              "3                          2   Aberdeen       WA  Washington  46.9757   \n",
              "4                          2   Aberdeen       WA  Washington  46.9757   \n",
              "\n",
              "        lng  population  density             timezone  \n",
              "0 -123.8094     16571.0    588.0  America/Los_Angeles  \n",
              "1 -123.8094     16571.0    588.0  America/Los_Angeles  \n",
              "2 -123.8094     16571.0    588.0  America/Los_Angeles  \n",
              "3 -123.8094     16571.0    588.0  America/Los_Angeles  \n",
              "4 -123.8094     16571.0    588.0  America/Los_Angeles  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "usAQi_df = pd.read_csv(\"US_AQI.csv\")\n",
        "usAQi_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based upon the above, the `geography_df` dataframe is constructed. This way, it allows to be in accordance with the main dataset, and allow the a focused approach to the following preprocessing steps. The dataframe includes geographical attributes such as area code, city, state, population, population density, and timezone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riz-AYblw4tz",
        "outputId": "15271012-8f91-4c28-af7a-5d3fc978eeac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "types Unnamed: 0                     int64\n",
            "CBSA Code                      int64\n",
            "Date                          object\n",
            "AQI                            int64\n",
            "Category                      object\n",
            "Defining Parameter            object\n",
            "Number of Sites Reporting      int64\n",
            "city_ascii                    object\n",
            "state_id                      object\n",
            "state_name                    object\n",
            "lat                          float64\n",
            "lng                          float64\n",
            "population                   float64\n",
            "density                      float64\n",
            "timezone                      object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# constructing geographic dataframe, columnized by the attributes: id, city , state, population, density, timezone\n",
        "# data types of columns are also checked to ensure they are in the correct format\n",
        "usAQi_df = pd.read_csv(\"US_AQI.csv\")\n",
        "print(\"types\", usAQi_df.dtypes)\n",
        "usAQi_df['Date'] = pd.to_datetime(usAQi_df['Date'])\n",
        "# usAQi_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "600xnyopw4tz",
        "outputId": "0f83a4f1-b37d-4858-9f41-13b3ded37d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5617325 5617325\n",
            "Area Code               int64\n",
            "City                   object\n",
            "State                  object\n",
            "Population              int64\n",
            "Population Density    float64\n",
            "Timezone               object\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area Code</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Population</th>\n",
              "      <th>Population Density</th>\n",
              "      <th>Timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10140</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>Washington</td>\n",
              "      <td>16571</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>10420</td>\n",
              "      <td>Akron</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>570375</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>10500</td>\n",
              "      <td>Albany</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>89323</td>\n",
              "      <td>509.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>10540</td>\n",
              "      <td>Albany</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>66405</td>\n",
              "      <td>1190.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>10580</td>\n",
              "      <td>Albany</td>\n",
              "      <td>New York</td>\n",
              "      <td>590823</td>\n",
              "      <td>1747.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Area Code      City       State  Population  Population Density  \\\n",
              "0        10140  Aberdeen  Washington       16571               588.0   \n",
              "31       10420     Akron        Ohio      570375              1230.0   \n",
              "121      10500    Albany     Georgia       89323               509.0   \n",
              "209      10540    Albany      Oregon       66405              1190.0   \n",
              "299      10580    Albany    New York      590823              1747.0   \n",
              "\n",
              "                Timezone  \n",
              "0    America/Los_Angeles  \n",
              "31      America/New_York  \n",
              "121     America/New_York  \n",
              "209  America/Los_Angeles  \n",
              "299     America/New_York  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "geography_df = pd.DataFrame()\n",
        "\n",
        "usCites = pd.read_csv(\"uscities.csv\")\n",
        "print(len(usAQi_df['city_ascii']), len(usAQi_df['state_name']))\n",
        "# add column \"Date\" for the geography dataframe and convert the column to datetime format\n",
        "usAQi_df[\"Date\"] = pd.to_datetime(usAQi_df[\"Date\"])\n",
        "\n",
        "# providing the column names for the geography dataframe\n",
        "geography_df['Area Code'] = usAQi_df['CBSA Code']\n",
        "geography_df[\"City\"] = usAQi_df['city_ascii']\n",
        "geography_df[\"State\"] = usAQi_df['state_name']\n",
        "geography_df[\"Population\"] = usAQi_df['population']\n",
        "geography_df[\"Population Density\"] = usAQi_df['density']\n",
        "geography_df[\"Timezone\"] = usAQi_df['timezone']\n",
        "\n",
        "\n",
        "# change type of population column to integer and check the data types of the columns in the geography_df dataframe\n",
        "geography_df[\"Population\"]= geography_df[\"Population\"].astype(int)\n",
        "print(geography_df.dtypes)\n",
        "\n",
        "geography_df = geography_df.drop_duplicates()\n",
        "geography_df.head()\n",
        "#remove NaN value\n",
        "# geography_df.dropna()\n",
        "# print(geography_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll8cPzSqw4tz"
      },
      "outputs": [],
      "source": [
        "# Generating surrogate keys for the geography dataframe and moving the column to the first column of the dataframe\n",
        "geography_df['Surrogate Keys'] = range(1,len(geography_df)+1)\n",
        "geography_df = geography_df.reindex(columns=['Surrogate Keys'] + list([c for c in geography_df.columns if c!= 'Surrogate Keys'])).reset_index()\n",
        "geography_df = geography_df.drop(\"index\",axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kzcMFisw4tz",
        "outputId": "46e574d4-c113-4ec0-e2fb-1fd8cd3343c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "671\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>Area Code</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Population</th>\n",
              "      <th>Population Density</th>\n",
              "      <th>Timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>10140</td>\n",
              "      <td>Aberdeen</td>\n",
              "      <td>Washington</td>\n",
              "      <td>16571</td>\n",
              "      <td>588.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>10420</td>\n",
              "      <td>Akron</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>570375</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>10500</td>\n",
              "      <td>Albany</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>89323</td>\n",
              "      <td>509.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>10540</td>\n",
              "      <td>Albany</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>66405</td>\n",
              "      <td>1190.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>10580</td>\n",
              "      <td>Albany</td>\n",
              "      <td>New York</td>\n",
              "      <td>590823</td>\n",
              "      <td>1747.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>10740</td>\n",
              "      <td>Albuquerque</td>\n",
              "      <td>New Mexico</td>\n",
              "      <td>762853</td>\n",
              "      <td>1155.0</td>\n",
              "      <td>America/Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>11100</td>\n",
              "      <td>Amarillo</td>\n",
              "      <td>Texas</td>\n",
              "      <td>202902</td>\n",
              "      <td>751.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>11140</td>\n",
              "      <td>Americus</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>15319</td>\n",
              "      <td>521.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>11460</td>\n",
              "      <td>Ann Arbor</td>\n",
              "      <td>Michigan</td>\n",
              "      <td>323593</td>\n",
              "      <td>1657.0</td>\n",
              "      <td>America/Detroit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>11540</td>\n",
              "      <td>Appleton</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>225728</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>11620</td>\n",
              "      <td>Ardmore</td>\n",
              "      <td>Oklahoma</td>\n",
              "      <td>24834</td>\n",
              "      <td>192.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>11660</td>\n",
              "      <td>Arkadelphia</td>\n",
              "      <td>Arkansas</td>\n",
              "      <td>10670</td>\n",
              "      <td>546.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>11780</td>\n",
              "      <td>Ashtabula</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>18067</td>\n",
              "      <td>898.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>11900</td>\n",
              "      <td>Athens</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>24978</td>\n",
              "      <td>973.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>12020</td>\n",
              "      <td>Athens</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>143081</td>\n",
              "      <td>414.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>12060</td>\n",
              "      <td>Atlanta</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>5151496</td>\n",
              "      <td>1419.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>12260</td>\n",
              "      <td>Augusta</td>\n",
              "      <td>Georgia</td>\n",
              "      <td>404125</td>\n",
              "      <td>252.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>12300</td>\n",
              "      <td>Augusta</td>\n",
              "      <td>Maine</td>\n",
              "      <td>18662</td>\n",
              "      <td>130.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>12420</td>\n",
              "      <td>Austin</td>\n",
              "      <td>Texas</td>\n",
              "      <td>1685522</td>\n",
              "      <td>1165.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>12540</td>\n",
              "      <td>Bakersfield</td>\n",
              "      <td>California</td>\n",
              "      <td>556892</td>\n",
              "      <td>979.0</td>\n",
              "      <td>America/Los_Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>12580</td>\n",
              "      <td>Baltimore</td>\n",
              "      <td>Maryland</td>\n",
              "      <td>2270087</td>\n",
              "      <td>2872.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>12620</td>\n",
              "      <td>Bangor</td>\n",
              "      <td>Maine</td>\n",
              "      <td>32029</td>\n",
              "      <td>360.0</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>12660</td>\n",
              "      <td>Baraboo</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>12133</td>\n",
              "      <td>637.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>12940</td>\n",
              "      <td>Baton Rouge</td>\n",
              "      <td>Louisiana</td>\n",
              "      <td>605449</td>\n",
              "      <td>993.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>13100</td>\n",
              "      <td>Beatrice</td>\n",
              "      <td>Nebraska</td>\n",
              "      <td>12282</td>\n",
              "      <td>500.0</td>\n",
              "      <td>America/Chicago</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Surrogate Keys  Area Code         City       State  Population  \\\n",
              "0                1      10140     Aberdeen  Washington       16571   \n",
              "1                2      10420        Akron        Ohio      570375   \n",
              "2                3      10500       Albany     Georgia       89323   \n",
              "3                4      10540       Albany      Oregon       66405   \n",
              "4                5      10580       Albany    New York      590823   \n",
              "5                6      10740  Albuquerque  New Mexico      762853   \n",
              "6                7      11100     Amarillo       Texas      202902   \n",
              "7                8      11140     Americus     Georgia       15319   \n",
              "8                9      11460    Ann Arbor    Michigan      323593   \n",
              "9               10      11540     Appleton   Wisconsin      225728   \n",
              "10              11      11620      Ardmore    Oklahoma       24834   \n",
              "11              12      11660  Arkadelphia    Arkansas       10670   \n",
              "12              13      11780    Ashtabula        Ohio       18067   \n",
              "13              14      11900       Athens        Ohio       24978   \n",
              "14              15      12020       Athens     Georgia      143081   \n",
              "15              16      12060      Atlanta     Georgia     5151496   \n",
              "16              17      12260      Augusta     Georgia      404125   \n",
              "17              18      12300      Augusta       Maine       18662   \n",
              "18              19      12420       Austin       Texas     1685522   \n",
              "19              20      12540  Bakersfield  California      556892   \n",
              "20              21      12580    Baltimore    Maryland     2270087   \n",
              "21              22      12620       Bangor       Maine       32029   \n",
              "22              23      12660      Baraboo   Wisconsin       12133   \n",
              "23              24      12940  Baton Rouge   Louisiana      605449   \n",
              "24              25      13100     Beatrice    Nebraska       12282   \n",
              "\n",
              "    Population Density             Timezone  \n",
              "0                588.0  America/Los_Angeles  \n",
              "1               1230.0     America/New_York  \n",
              "2                509.0     America/New_York  \n",
              "3               1190.0  America/Los_Angeles  \n",
              "4               1747.0     America/New_York  \n",
              "5               1155.0       America/Denver  \n",
              "6                751.0      America/Chicago  \n",
              "7                521.0     America/New_York  \n",
              "8               1657.0      America/Detroit  \n",
              "9               1150.0      America/Chicago  \n",
              "10               192.0      America/Chicago  \n",
              "11               546.0      America/Chicago  \n",
              "12               898.0     America/New_York  \n",
              "13               973.0     America/New_York  \n",
              "14               414.0     America/New_York  \n",
              "15              1419.0     America/New_York  \n",
              "16               252.0     America/New_York  \n",
              "17               130.0     America/New_York  \n",
              "18              1165.0      America/Chicago  \n",
              "19               979.0  America/Los_Angeles  \n",
              "20              2872.0     America/New_York  \n",
              "21               360.0     America/New_York  \n",
              "22               637.0      America/Chicago  \n",
              "23               993.0      America/Chicago  \n",
              "24               500.0      America/Chicago  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(geography_df.shape[0])\n",
        "geography_df.head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct the `pollutant_df` dataframe, the poulltant datasets each need to be processed and analysed thoroughly. This is to establish a universal convention for all pollutants within the data mart, with each providing data based on the same ID (in this case, all can be linked via the `Date` attribute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEUu5dyrw4t0",
        "outputId": "3e6afc55-afcd-4c64-957d-8c0a6c23d63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "files length(should be same): 43 43 43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1q/tg3j4nxj1r15d72yljd9qj9h0000gn/T/ipykernel_76062/2777298108.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  pm2_5_df = pd.concat(dataframes_pm2_5, ignore_index=True)\n",
            "/var/folders/1q/tg3j4nxj1r15d72yljd9qj9h0000gn/T/ipykernel_76062/2777298108.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  pm10_df = pd.concat(dataframes_pm10, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "folder_path = './NO2'\n",
        "\n",
        "# List all files and directories in the folder\n",
        "files = os.listdir(folder_path) # folder path for NO2\n",
        "files_co = os.listdir(\"./CO\")\n",
        "files_so2 = os.listdir(\"./SO2\")\n",
        "files_o3 = os.listdir(\"./O3\")\n",
        "files_pm2_5 = os.listdir(\"./pm2.5\")\n",
        "files_pm10 = os.listdir(\"./pm10\")\n",
        "\n",
        "print(\"files length(should be same):\",len(files),len(files_co),len(files_so2))\n",
        "files.remove(\".DS_Store\")\n",
        "# files_co.remove(\".DS_Store\")\n",
        "dataframes = []\n",
        "dataframes_co=[]\n",
        "dataframes_so2 =[]\n",
        "dataframes_o3 =[]\n",
        "dataframes_pm2_5=[]\n",
        "dataframes_pm10=[]\n",
        "\n",
        "# Read each CSV into a DataFrame and store and append them in a list\n",
        "for i in range(len(files)):\n",
        "    df = pd.read_csv(f\"./NO2/{files[i]}\",low_memory=False)\n",
        "    df2 = pd.read_csv(f\"./CO/{files_co[i]}\",low_memory=False)\n",
        "    df3 = pd.read_csv((f\"./SO2/{files_so2[i]}\"),low_memory=False)\n",
        "    df4 = pd.read_csv((f\"./O3/{files_o3[i]}\"),low_memory=False)\n",
        "    df5 = pd.read_csv((f\"./pm2.5//{files_pm2_5[i]}\"),low_memory=False)\n",
        "    df6 = pd.read_csv((f\"./pm10/{files_pm10[i]}\"),low_memory=False)\n",
        "\n",
        "    # Extract station name from filename (remove '.csv' part)\n",
        "    # station_name = file[:-4]  # This removes the last 4 characters, assuming they are '.csv'\n",
        "    # Add a new column with the station name\n",
        "    # df['station_id'] = station_name\n",
        "    dataframes.append(df)\n",
        "    dataframes_co.append(df2)\n",
        "    dataframes_so2.append(df3)\n",
        "    dataframes_o3.append(df4)\n",
        "    dataframes_pm2_5.append(df5)\n",
        "    dataframes_pm10.append(df6)\n",
        "\n",
        "no2_df = pd.concat(dataframes, ignore_index=True)\n",
        "co_df = pd.concat(dataframes_co, ignore_index=True)\n",
        "so2_df = pd.concat(dataframes_so2, ignore_index=True)\n",
        "o3_df = pd.concat(dataframes_o3, ignore_index=True)\n",
        "pm2_5_df = pd.concat(dataframes_pm2_5, ignore_index=True)\n",
        "pm10_df = pd.concat(dataframes_pm10, ignore_index=True)\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To faciliate the operation for all pollutants, this is a helper function that takes a DataFrame and a pollutant name as input. It then creates a new DataFrame with specific columns from the input DataFrame, renames the `Arithmetic Mean` column to the name of the pollutant, converts the `Date Local` column to datetime format, and removes rows where the city is `Not in a city`. It returns the processed DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHMKynoRw4t0"
      },
      "outputs": [],
      "source": [
        "def process_pollutant_df(df, pollutant_name):\n",
        "    # new_df = df[[\"Date Local\", \"Arithmetic Mean\", \"State Name\", \"Sample Duration\", \"City Name\"]]\n",
        "    new_df = pd.DataFrame()\n",
        "    new_df['Date'] = df['Date Local']\n",
        "    new_df['State'] = df['State Name']\n",
        "    new_df['City'] = df['City Name']\n",
        "    new_df[\"Sample Duration\"] = df[\"Sample Duration\"]\n",
        "    new_df[f\"{pollutant_name} Mean\"] = df[\"Arithmetic Mean\"]\n",
        "    new_df['Date'] = pd.to_datetime(df['Date Local'])  # Convert 'Date Local' to datetime\n",
        "    new_df = new_df[new_df['City'] != \"Not in a city\"]  # Drop rows where 'City Name' is \"Not in a city\"\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the pollutants data for easier formatting and further analysis, the following code bloc cleans and processes the data, and also checks for missing values which can affect the accuracy of the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcAJaQRZw4t0",
        "outputId": "7f241e97-eea2-4dd5-9973-9981bec01f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "NO2 DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "NO2 Mean                  float64\n",
            "dtype: object\n",
            "NO2 DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "NO2 Mean           0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "CO DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "CO Mean                   float64\n",
            "dtype: object\n",
            "CO DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "CO Mean            0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "SO2 DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "SO2 Mean                  float64\n",
            "dtype: object\n",
            "SO2 DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "SO2 Mean           0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "O3 DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "O3 Mean                   float64\n",
            "dtype: object\n",
            "O3 DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "O3 Mean            0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "PM2.5 DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "PM2.5 Mean                float64\n",
            "dtype: object\n",
            "PM2.5 DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "PM2.5 Mean         0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [State Code, County Code, Site Num, Parameter Code, POC, Latitude, Longitude, Datum, Parameter Name, Sample Duration, Pollutant Standard, Date Local, Units of Measure, Event Type, Observation Count, Observation Percent, Arithmetic Mean, 1st Max Value, 1st Max Hour, AQI, Method Code, Method Name, Local Site Name, Address, State Name, County Name, City Name, CBSA Name, Date of Last Change]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 29 columns]\n",
            "PM10 DataFrame dtypes:\n",
            "Date               datetime64[ns]\n",
            "State                      object\n",
            "City                       object\n",
            "Sample Duration            object\n",
            "PM10 Mean                 float64\n",
            "dtype: object\n",
            "PM10 DataFrame NaN values:\n",
            "Date               0\n",
            "State              0\n",
            "City               0\n",
            "Sample Duration    0\n",
            "PM10 Mean          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#create a polluatant data frame and filter pollutant data\n",
        "pollutant_df = pd.DataFrame()\n",
        "# create a dictionary to store the pollutant dataframes: keys are the pollutant names and values are the pollutant dataframes\n",
        "pollutant_dfs = {\n",
        "    'NO2': no2_df,\n",
        "    'CO': co_df,\n",
        "    'SO2': so2_df,\n",
        "    'O3': o3_df,\n",
        "    'PM2.5': pm2_5_df,\n",
        "    'PM10': pm10_df\n",
        "}\n",
        "\n",
        "# Process each dataframe\n",
        "for pollutant, df in pollutant_dfs.items():\n",
        "    print(df.head(0))\n",
        "    pollutant_dfs[pollutant] = process_pollutant_df(df, pollutant)\n",
        "\n",
        "    # Print datatype information\n",
        "    print(f\"{pollutant} DataFrame dtypes:\")\n",
        "    print(pollutant_dfs[pollutant].dtypes)\n",
        "\n",
        "    # Check for NaN values\n",
        "    print(f\"{pollutant} DataFrame NaN values:\")\n",
        "    print(pollutant_dfs[pollutant].isna().sum())\n",
        "\n",
        "no2_df = pollutant_dfs['NO2']\n",
        "co_df = pollutant_dfs['CO']\n",
        "so2_df = pollutant_dfs['SO2']\n",
        "o3_df = pollutant_dfs['O3']\n",
        "pm10_df = pollutant_dfs['PM10']\n",
        "pm2_5_df = pollutant_dfs['PM2.5']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JP5Qlmxw4t1"
      },
      "source": [
        "Keep one variation of the sample duration and then drop the column `Sample Duration` as it was only needed for filteration. To be specific, the pollutant data is further cleaned by removing unnecessary rows and columns. The filtering is necessary because the analysis may require data that was sampled at specific intervals (e.g., every 1 hour, every 8 hours, or every 24 hours). The column dropping is necessary to simplify the DataFrames and reduce memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pta9lAY5w4t1"
      },
      "outputs": [],
      "source": [
        "# clean the repeated value measure and take the 8hour average\n",
        "# print(pm10_df[\"Sample Duration\"].unique())\n",
        "# print(co_df[co_df['State Name'] == \"Arizona\"].shape[0])\n",
        "filtered_co = co_df[co_df['Sample Duration'] == \"1 HOUR\"]\n",
        "filtered_no2 = no2_df[no2_df['Sample Duration'] == \"1 HOUR\"]\n",
        "filtered_so2 = so2_df[so2_df['Sample Duration'] == \"1 HOUR\"]\n",
        "filtered_o3 = o3_df[o3_df['Sample Duration'] == \"8-HR RUN AVG BEGIN HOUR\"]\n",
        "filtered_pm2_5 = pm2_5_df[pm2_5_df['Sample Duration'] == \"1 HOUR\"]\n",
        "filtered_pm10 = pm10_df[pm10_df['Sample Duration'] == \"24 HOUR\"]\n",
        "\n",
        "\n",
        "# print(filtered_o3.shape[0])\n",
        "# drop sample coloum\n",
        "filtered_co=filtered_co.drop(\"Sample Duration\", axis=1)\n",
        "filtered_no2=filtered_no2.drop(\"Sample Duration\", axis=1)\n",
        "filtered_so2=filtered_so2.drop(\"Sample Duration\", axis=1)\n",
        "filtered_o3=filtered_o3.drop(\"Sample Duration\", axis=1)\n",
        "filtered_pm2_5 = filtered_pm2_5.drop(\"Sample Duration\", axis=1)\n",
        "filtered_pm10=filtered_pm10.drop(\"Sample Duration\", axis=1)\n",
        "\n",
        "# print(filtered_co.shape[0])\n",
        "# print(filtered_co.head(25))\n",
        "# print(filtered_no2.shape[0])\n",
        "# print(filtered_o3.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-m9Iiww4t1"
      },
      "source": [
        "Now joining all the pollutants using **outer join** since we want to know which row result in null. Merging all the pollutant DataFrames into a single DataFrame for easier analysis, simplifying the data structure and makings it easier to perform analysis on all the pollutants together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCf2WiRsw4t2"
      },
      "outputs": [],
      "source": [
        "# join the polluant using inner join on date\n",
        "# create a list of the filtered pollutant DataFrame\n",
        "filtered_pollutant_list = [filtered_co, filtered_no2, filtered_so2, filtered_o3, filtered_pm2_5, filtered_pm10]\n",
        "\n",
        "# initialize as the first DataFrame in the list\n",
        "joined_final = filtered_pollutant_list[0]\n",
        "\n",
        "# Loop through the list and merge each DataFrame to the final DataFrame\n",
        "for df in filtered_pollutant_list[1:]:\n",
        "    # print(df.head(0))\n",
        "    joined_final = joined_final.merge(df, on=['City', 'State', 'Date'], how=\"outer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOJEcEXSw4t2",
        "outputId": "70b38754-324c-45d0-f186-e83c47bfbc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Date         State          City   CO Mean   NO2 Mean  SO2 Mean  \\\n",
            "0 1980-01-01          Iowa     Davenport  0.000000        NaN       NaN   \n",
            "1 1980-01-01  Pennsylvania  Philadelphia  3.833333  46.666667      17.5   \n",
            "\n",
            "    O3 Mean  PM2.5 Mean  PM10 Mean  \n",
            "0       NaN         NaN        NaN  \n",
            "1  0.004294         NaN        NaN  \n"
          ]
        }
      ],
      "source": [
        "# Sort, reset index, and display the head of the final DataFrame\n",
        "joined_final = joined_final.sort_values(by='Date')\n",
        "joined_final = joined_final.reset_index()\n",
        "joined_final = joined_final.drop(\"index\", axis=1)\n",
        "\n",
        "# Display the head of the final DataFrame\n",
        "print(joined_final.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6hihl4w4t2"
      },
      "source": [
        "Since there are many oberservations done, they are grouped as `[DATE, STATE, CITY]`, then averaged by the mean on the pollutant value. All pollutant measurements are converted to be of the same units as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht6qRMg0w4t2",
        "outputId": "dc592a5a-57a3-4347-ac70-bcbdf9e2bd4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147091898\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missig Rows for CO % 7.663634199621246\n",
            "Missig Rows for NO2 % 8.75496827160392\n",
            "Missig Rows for SO2 % 7.514538292245029\n",
            "Missig Rows for O3 % 0.062197450195387374\n",
            "Missing Rows for PM 2.5 % 40.349170013429294\n",
            "Missing Rows for PM 10 % 49.897546362478785\n",
            "                      City           State       Date  NO2 Mean   CO Mean  \\\n",
            "9139466             Peoria        Illinois 1980-01-01       NaN  0.462500   \n",
            "2647209         Costa Mesa      California 1980-01-01   0.04087  4.208333   \n",
            "10101561         Rock Hill  South Carolina 1980-01-01       NaN  0.891667   \n",
            "1366981            Bristol        Virginia 1980-01-01       NaN       NaN   \n",
            "12455180            Warren            Ohio 1980-01-01       NaN       NaN   \n",
            "4470756          Glen Cove        New York 1980-01-01       NaN       NaN   \n",
            "13019964  Wisconsin Rapids       Wisconsin 1980-01-01       NaN  0.529167   \n",
            "12558497          Waukesha       Wisconsin 1980-01-01       NaN  0.300000   \n",
            "2431362           Columbia  South Carolina 1980-01-01       NaN  0.816667   \n",
            "2953214          Deerfield        Illinois 1980-01-01       NaN       NaN   \n",
            "\n",
            "          SO2 Mean   O3 Mean  PM2.5 Mean  PM10 Mean  \n",
            "9139466   0.000375  0.008706         NaN        NaN  \n",
            "2647209   0.004583  0.004412         NaN        NaN  \n",
            "10101561       NaN  0.003118         NaN        NaN  \n",
            "1366981   0.035417       NaN         NaN        NaN  \n",
            "12455180  0.008696       NaN         NaN        NaN  \n",
            "4470756   0.014917       NaN         NaN        NaN  \n",
            "13019964       NaN       NaN         NaN        NaN  \n",
            "12558497       NaN       NaN         NaN        NaN  \n",
            "2431362        NaN  0.007294         NaN        NaN  \n",
            "2953214        NaN  0.010000         NaN        NaN  \n",
            "13235772\n"
          ]
        }
      ],
      "source": [
        "print(joined_final.shape[0])\n",
        "# print(joined_final.isna().sum())\n",
        "print(\"Missing Rows for CO %\",joined_final['CO Mean'].isna().sum()/joined_final.shape[0]*100)\n",
        "print(\"Missing Rows for NO2 %\",joined_final['NO2 Mean'].isna().sum()/joined_final.shape[0]*100)\n",
        "print(\"Missing Rows for SO2 %\",joined_final['SO2 Mean'].isna().sum()/joined_final.shape[0] *100)\n",
        "print(\"Missing Rows for O3 %\",joined_final['O3 Mean'].isna().sum()/joined_final.shape[0])\n",
        "\n",
        "print('Missing Rows for PM 2.5 %',joined_final['PM2.5 Mean'].isna().sum()/joined_final.shape[0]*100)\n",
        "print('Missing Rows for PM 10 %',joined_final['PM10 Mean'].isna().sum()/joined_final.shape[0]*100)\n",
        "\n",
        "\n",
        "df_sorted_by_date = joined_final.sort_values('Date')\n",
        "# print(df_sorted_by_date.head(10))\n",
        "grouped_df = df_sorted_by_date.groupby(['City', 'State', 'Date']).agg({\n",
        "    'NO2 Mean': 'mean',\n",
        "    'CO Mean':'mean',\n",
        "    'SO2 Mean': 'mean',\n",
        "    'O3 Mean': 'mean',\n",
        "    'PM2.5 Mean':'mean',\n",
        "    'PM10 Mean':'mean'\n",
        "    # Add more columns and their aggregation functions as needed\n",
        "}).reset_index()\n",
        "grouped_df = grouped_df.sort_values('Date')\n",
        "# convert ppb and g/m3 to ppm for consistent units\n",
        "grouped_df['NO2 Mean'] = grouped_df['NO2 Mean'] / 1000\n",
        "grouped_df['SO2 Mean'] = grouped_df['SO2 Mean'] / 1000\n",
        "grouped_df['PM10 Mean'] = grouped_df['PM10 Mean']/1000\n",
        "grouped_df['PM2.5 Mean'] = grouped_df['PM2.5 Mean']/1000\n",
        "\n",
        "print(grouped_df.head(10))\n",
        "print(grouped_df.shape[0])\n",
        "pollutant_df = grouped_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle the missing values problem in the pollutant data, the **time-based interpolation** is a reasonable method for filling missing values in time series data because it assumes that the data follows a linear trend over time. As such, the accuracy of the analysis is not diminished, but rather favoured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFiOZHA4w4t3",
        "outputId": "2d6a7889-9e54-4d0f-9498-80fd6acb4ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     City           State  NO2 Mean   CO Mean  SO2 Mean  \\\n",
            "Date                                                                      \n",
            "1980-01-01         Peoria        Illinois  0.000000  0.462500  0.000375   \n",
            "1980-01-01     Costa Mesa      California  0.040870  4.208333  0.004583   \n",
            "1980-01-01      Rock Hill  South Carolina  0.000000  0.891667  0.021542   \n",
            "1980-01-01        Bristol        Virginia  0.000000  3.645833  0.035417   \n",
            "1980-01-01         Warren            Ohio  0.000000  3.645833  0.008696   \n",
            "...                   ...             ...       ...       ...       ...   \n",
            "1986-07-10  San Francisco      California  0.016458  1.500000  0.005833   \n",
            "1986-07-10      Pawtucket    Rhode Island  0.015909  1.500000  0.002100   \n",
            "1986-07-10      Hollister      California  0.015909  1.500000  0.003043   \n",
            "1986-07-10   Granite City        Illinois  0.015909  1.333333  0.003043   \n",
            "1986-07-10         Newark      New Jersey  0.015750  0.616667  0.004625   \n",
            "\n",
            "             O3 Mean  PM2.5 Mean  PM10 Mean  \n",
            "Date                                         \n",
            "1980-01-01  0.008706    0.015688      0.024  \n",
            "1980-01-01  0.004412    0.015688      0.024  \n",
            "1980-01-01  0.003118    0.015688      0.024  \n",
            "1980-01-01  0.003000    0.015688      0.024  \n",
            "1980-01-01  0.003000    0.015688      0.024  \n",
            "...              ...         ...        ...  \n",
            "1986-07-10  0.008235    0.015688      0.009  \n",
            "1986-07-10  0.018059    0.015688      0.009  \n",
            "1986-07-10  0.026588    0.015688      0.009  \n",
            "1986-07-10  0.018059    0.015688      0.072  \n",
            "1986-07-10  0.018059    0.015688      0.009  \n",
            "\n",
            "[1600000 rows x 8 columns]\n",
            "13235772\n",
            "Missig Rows for CO % 0.0\n",
            "Missig Rows for NO2 % 0.0\n",
            "Missig Rows for SO2 % 0.0\n",
            "Missig Rows for O3 % 0.0\n",
            "Missing Rows for PM 2.5 % 0.0\n",
            "Missing Rows for PM 10 % 0.0\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.DataFrame()\n",
        "test_df = pollutant_df.copy()\n",
        "# print(test_df.head())\n",
        "# print(pollutant_df.head())\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "test_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Perform time-based interpolation\n",
        "# interpolates missing values in each pollutant column based on time.\n",
        "# The 'both' limit direction means that missing values at the beginning and end of the series are filled as well\n",
        "for column in ['NO2 Mean', 'CO Mean', 'SO2 Mean', 'O3 Mean', 'PM2.5 Mean', 'PM10 Mean']:\n",
        "    test_df[column].interpolate(method='time', inplace=True, limit_direction='both')\n",
        "print(test_df.head(1600000))\n",
        "\n",
        "# print the number of missing values in each column after interpolation\n",
        "print(test_df.shape[0])\n",
        "# print(joined_final.isna().sum())\n",
        "print(\"Missing Rows for CO %\",test_df['CO Mean'].isna().sum()/test_df.shape[0]*100)\n",
        "print(\"Missing Rows for NO2 %\",test_df['NO2 Mean'].isna().sum()/test_df.shape[0]*100)\n",
        "print(\"Missing Rows for SO2 %\",test_df['SO2 Mean'].isna().sum()/test_df.shape[0] *100)\n",
        "print(\"Missing Rows for O3 %\",test_df['O3 Mean'].isna().sum()/test_df.shape[0]*100)\n",
        "\n",
        "print('Missing Rows for PM 2.5 %',test_df['PM2.5 Mean'].isna().sum()/test_df.shape[0]*100)\n",
        "print('Missing Rows for PM 10 %',test_df['PM10 Mean'].isna().sum()/test_df.shape[0]*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2kVfF7Sw4t4"
      },
      "outputs": [],
      "source": [
        "# Add the Risk level to the pollutant based on the main dataset for the each date\n",
        "pollutant_df = test_df\n",
        "pollutant_df = pd.merge(pollutant_df, usAQi_df , left_on=['State','City','Date'],right_on=['state_name','city_ascii','Date'],how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAKJ9gK-w4t4",
        "outputId": "eea6dfdc-c159-4ff0-874b-4c7e998e532e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>NO2 Mean</th>\n",
              "      <th>CO Mean</th>\n",
              "      <th>SO2 Mean</th>\n",
              "      <th>O3 Mean</th>\n",
              "      <th>PM2.5 Mean</th>\n",
              "      <th>PM10 Mean</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>Peoria</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>California</td>\n",
              "      <td>Costa Mesa</td>\n",
              "      <td>0.04087</td>\n",
              "      <td>4.208333</td>\n",
              "      <td>0.004583</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>South Carolina</td>\n",
              "      <td>Rock Hill</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.891667</td>\n",
              "      <td>0.021542</td>\n",
              "      <td>0.003118</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>Bristol</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>3.645833</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>Warren</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>3.645833</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date           State        City  NO2 Mean   CO Mean  SO2 Mean  \\\n",
              "0 1980-01-01        Illinois      Peoria   0.00000  0.462500  0.000375   \n",
              "1 1980-01-01      California  Costa Mesa   0.04087  4.208333  0.004583   \n",
              "2 1980-01-01  South Carolina   Rock Hill   0.00000  0.891667  0.021542   \n",
              "3 1980-01-01        Virginia     Bristol   0.00000  3.645833  0.035417   \n",
              "4 1980-01-01            Ohio      Warren   0.00000  3.645833  0.008696   \n",
              "\n",
              "    O3 Mean  PM2.5 Mean  PM10 Mean Category  \n",
              "0  0.008706    0.015688      0.024     Good  \n",
              "1  0.004412    0.015688      0.024      NaN  \n",
              "2  0.003118    0.015688      0.024      NaN  \n",
              "3  0.003000    0.015688      0.024      NaN  \n",
              "4  0.003000    0.015688      0.024      NaN  "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reassigning the columns of the pollutant dataframe to the desired order and display the head of the dataframe\n",
        "pollutant_df = pollutant_df[['Date','State','City','NO2 Mean',\"CO Mean\", \"SO2 Mean\",\"O3 Mean\",\"PM2.5 Mean\",\"PM10 Mean\",\"Category\"]]\n",
        "pollutant_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_iJp38Mw4t5",
        "outputId": "388ba371-cecd-45c9-fed5-aa6d3d4336da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>Date</th>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>NO2 Mean</th>\n",
              "      <th>CO Mean</th>\n",
              "      <th>SO2 Mean</th>\n",
              "      <th>O3 Mean</th>\n",
              "      <th>PM2.5 Mean</th>\n",
              "      <th>PM10 Mean</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>Peoria</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>California</td>\n",
              "      <td>Costa Mesa</td>\n",
              "      <td>0.04087</td>\n",
              "      <td>4.208333</td>\n",
              "      <td>0.004583</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>South Carolina</td>\n",
              "      <td>Rock Hill</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.891667</td>\n",
              "      <td>0.021542</td>\n",
              "      <td>0.003118</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>Bristol</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>3.645833</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1980-01-01</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>Warren</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>3.645833</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys       Date           State        City  NO2 Mean   CO Mean  \\\n",
              "0               1 1980-01-01        Illinois      Peoria   0.00000  0.462500   \n",
              "1               2 1980-01-01      California  Costa Mesa   0.04087  4.208333   \n",
              "2               3 1980-01-01  South Carolina   Rock Hill   0.00000  0.891667   \n",
              "3               4 1980-01-01        Virginia     Bristol   0.00000  3.645833   \n",
              "4               5 1980-01-01            Ohio      Warren   0.00000  3.645833   \n",
              "\n",
              "   SO2 Mean   O3 Mean  PM2.5 Mean  PM10 Mean Category  \n",
              "0  0.000375  0.008706    0.015688      0.024     Good  \n",
              "1  0.004583  0.004412    0.015688      0.024      NaN  \n",
              "2  0.021542  0.003118    0.015688      0.024      NaN  \n",
              "3  0.035417  0.003000    0.015688      0.024      NaN  \n",
              "4  0.008696  0.003000    0.015688      0.024      NaN  "
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generating surrogate keys for the pollutant dataframe and moving the column to the first column of the dataframe\n",
        "pollutant_df['Surrogate Keys'] = range(1,len(pollutant_df)+1)\n",
        "pollutant_df = pollutant_df.reindex(columns=['Surrogate Keys'] + list([c for c in pollutant_df.columns if c!= 'Surrogate Keys']))\n",
        "pollutant_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywxo2mxYw4t9"
      },
      "outputs": [],
      "source": [
        "# to write to csv\n",
        "# df.to_csv(file_name, encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point on, the merging process begins, where a certain pair of dataframes are joined based on a common attribute. Before doing so, some further preprocessing is required, to ease the operations and perform in an effective and effecient manner. First up is to merge the pollutant and geographical data into a single DataFrame. This allows the user to analyze the pollutant data in the context of geographical information. In addition, the calculation of the number and percentage of deleted rows is useful for understanding the impact of the merge operation on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy0xMlNow4t9",
        "outputId": "3ed69924-e671-4db0-e488-e026f0246e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3930979\n",
            "number of rows deleted from pollutant: 9304793 70.30034213342448 %\n",
            "   Surrogate Keys_x       Date     State    City  NO2 Mean   CO Mean  \\\n",
            "0                 1 1980-01-01  Illinois  Peoria       NaN  0.462500   \n",
            "1               688 1980-01-02  Illinois  Peoria       NaN  0.858333   \n",
            "2              1651 1980-01-03  Illinois  Peoria       NaN  0.770000   \n",
            "3              1897 1980-01-04  Illinois  Peoria       NaN  1.266667   \n",
            "4              2420 1980-01-05  Illinois  Peoria       NaN  1.137500   \n",
            "\n",
            "   SO2 Mean   O3 Mean  PM2.5 Mean  PM10 Mean Category  Surrogate Keys_y  \\\n",
            "0  0.000375  0.008706         NaN        NaN     Good               231   \n",
            "1  0.001208  0.002529         NaN        NaN     Good               231   \n",
            "2  0.006987  0.004235         NaN        NaN     Good               231   \n",
            "3  0.010042  0.003824         NaN        NaN     Good               231   \n",
            "4  0.008063  0.008647         NaN        NaN     Good               231   \n",
            "\n",
            "   Area Code  Population  Population Density         Timezone  \n",
            "0      37900      253461               906.0  America/Chicago  \n",
            "1      37900      253461               906.0  America/Chicago  \n",
            "2      37900      253461               906.0  America/Chicago  \n",
            "3      37900      253461               906.0  America/Chicago  \n",
            "4      37900      253461               906.0  America/Chicago  \n"
          ]
        }
      ],
      "source": [
        "# stores the number of rows in pollutant_df before merging with geography_df\n",
        "row_tmp_sum1 = pollutant_df.shape[0]\n",
        "# merges pollutant_df and geography_df based on the 'City' and 'State' columns\n",
        "# The result is a DataFrame that includes both pollutant and geographical information for each city and state.\n",
        "merge_pollutant_geo = pollutant_df.merge(geography_df, left_on=['City', 'State',], right_on=['City', 'State', ])\n",
        "\n",
        "# merge_pollutant_geo = merge_pollutant_geo.drop([\"population\",\"population_density\",\"timezone\",'City', 'State'],axis=1)\n",
        "# This stores the number of rows in merge_pollutant_geo after merging with geography_df\n",
        "row_tmp_sum2 = (merge_pollutant_geo.shape[0])\n",
        "# print(type(row_tmp_sum1))\n",
        "print(row_tmp_sum2)\n",
        "print(\"number of rows deleted from pollutant:\" ,(row_tmp_sum1 - row_tmp_sum2 ), ((row_tmp_sum1 - row_tmp_sum2)/row_tmp_sum1)*100,\"%\" )\n",
        "print(merge_pollutant_geo.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8w5Gx-Sw4t-",
        "outputId": "82473252-4755-4a51-ba81-be44ea7549f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[] []\n"
          ]
        }
      ],
      "source": [
        "# print(jointest.head())\n",
        "# merging on the 'City' and 'State' columns\n",
        "merged_df = pd.merge(pollutant_df, geography_df, how='left', left_on=['City', 'State'], right_on=['City', 'State'])\n",
        "\n",
        "# Filter out the rows where the join from df2 resulted in NaN values\n",
        "non_matching_rows = merged_df[merged_df['City'].isnull() | merged_df['State'].isnull()]\n",
        "\n",
        "print(non_matching_rows['State'].unique(), non_matching_rows['City'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgM-NzWMw4t_",
        "outputId": "1137972d-5243-496d-d523-a8dfee2c030e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Surrogate Keys_x, Date, State, City, NO2 Mean, CO Mean, SO2 Mean, O3 Mean, PM2.5 Mean, PM10 Mean, Category, Surrogate Keys_y, Area Code, Population, Population Density, Timezone]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "#rows that were removed when merging pollutant and geo\n",
        "print(non_matching_rows.head(800))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeO5qlNFw4t_"
      },
      "source": [
        "With the Google Maps API, this method performs reverse geocoding, which converts geographic coordinates into a human-readable address. It then extracts the `city` and `state` from the address components and returns them (latitude and longitude --> city and state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euYPxZlbw4uA"
      },
      "outputs": [],
      "source": [
        "def get_city_state(lat, lon):\n",
        "    reverse_geocode_result = gmaps.reverse_geocode((lat, lon))\n",
        "    city, state = None, None\n",
        "    for component in reverse_geocode_result[0]['address_components']:\n",
        "        if 'locality' in component['types']:\n",
        "            city = component['long_name']\n",
        "        elif 'administrative_area_level_1' in component['types']:\n",
        "            state = component['long_name']\n",
        "    return city, state\n",
        "\n",
        "# Create a wrapper function to apply on the DataFrame\n",
        "def apply_get_city_state(row):\n",
        "    return get_city_state(row['Lat'], row['Lon'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruLhQXutw4uA"
      },
      "outputs": [],
      "source": [
        "# Apply the above function to the Station DataFrame to get the city and state for each row\n",
        "city_state_df = station_df.apply(apply_get_city_state, axis=1, result_type='expand')\n",
        "city_state_df.columns = ['City', 'State']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A key note here to distinguish the different merging techniques is that `merge()` is used to combine two (or more) dataframes on the basis of values of common columns (as used via *indices*: `left_index=True` and/or `right_index=True`), and `concat()` is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1). `join()` is used to merge 2 dataframes on the basis of the index; instead of using `merge()` with the option `left_index=True` we can use `join()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuYxw6CHw4uA",
        "outputId": "be7e66d7-701a-4abe-de33-23e5f5672d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Surrogate Keys           ID      Lat       Lon  Status            City  \\\n",
            "0                 1  USC00042863  33.1211 -117.0900  Active       Escondido   \n",
            "1                 2  USC00166584  31.8142  -93.0856  Active    Natchitoches   \n",
            "2                 3  USC00280734  40.8292  -75.0833  Active       Belvidere   \n",
            "3                 4  USC00286055  40.4728  -74.4225  Active  East Brunswick   \n",
            "4                 5  USC00356749  45.5181 -122.6894  Active        Portland   \n",
            "..              ...          ...      ...       ...     ...             ...   \n",
            "205             206  USW00094910  42.5544  -92.4011  Active        Waterloo   \n",
            "206             207  USW00094929  46.5486  -90.9189  Active         Ashland   \n",
            "207             208  USW00094949  40.6044  -98.4272  Active        Hastings   \n",
            "208             209  USW00094950  43.7667  -98.0333  Active        Mitchell   \n",
            "209             210  USW00094973  46.0261  -91.4442  Active         Hayward   \n",
            "\n",
            "            State  \n",
            "0      California  \n",
            "1       Louisiana  \n",
            "2      New Jersey  \n",
            "3      New Jersey  \n",
            "4          Oregon  \n",
            "..            ...  \n",
            "205          Iowa  \n",
            "206     Wisconsin  \n",
            "207      Nebraska  \n",
            "208  South Dakota  \n",
            "209     Wisconsin  \n",
            "\n",
            "[210 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Concatenate the city_state_df DataFrame with the station_df DataFrame\n",
        "station_df = pd.concat([station_df, city_state_df], axis=1)\n",
        "station_df = station_df.drop(\"city_name\", axis=1)\n",
        "print(station_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This new dataframe regarding `date_main` is to extract the granular components of the main dataset, so that all other dataframes are to be in sync with respect to these `date` attribute components (since the measures are availbale for a certain period of time ranges)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0X-26d4w4uB",
        "outputId": "c1b1c3d0-7c86-467a-f5b1-a5cc53e42937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15492"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filter by dates\n",
        "\n",
        "# Extract and sort the record based on assencding\n",
        "date_main = pd.DataFrame()\n",
        "# Extract year, month, and day\n",
        "date_main['day'] = usAQi_df['Date'].dt.day\n",
        "date_main['month'] = usAQi_df['Date'].dt.month\n",
        "\n",
        "date_main['year'] = usAQi_df['Date'].dt.year\n",
        "\n",
        "# date_df['season'] = getSeason(date_df['month'])\n",
        "\n",
        "date_main['date'] = pd.to_datetime(date_main[['year', 'month', 'day']])\n",
        "\n",
        "# Calculate the day of the week and assign it to a new column\n",
        "date_main['day_of_week'] = date_main['date'].dt.day_name()\n",
        "date_main['season'] = date_main['month'].apply(get_season)\n",
        "\n",
        "date_main = date_main.drop(\"date\", axis=1)\n",
        "date_main = date_main.drop_duplicates() # drop duplicate dates\n",
        "date_main = date_main.sort_values(by=['year', 'month', 'day'])\n",
        "\n",
        "# Generating surrogate keys for the date dataframe and moving the column to the first column of the dataframe\n",
        "\n",
        "date_main['Surrogate Keys'] = range(1,len(date_main)+1)\n",
        "date_main = date_main.reindex(columns=['Surrogate Keys'] + list([c for c in date_main.columns if c!= 'Surrogate Keys']))\n",
        "\n",
        "# Reset and drop the generated (extra) index column\n",
        "date_main = date_main.reset_index()\n",
        "date_main = date_main.drop(\"index\",axis=1)\n",
        "date_main.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other dataframes below are created as filtered versions of the datasets, taking less space in storage while achieving the same purpose. They are then to be merged all together as the final dataset to be used for the loading step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7mE3Rt8w4uB"
      },
      "outputs": [],
      "source": [
        "# filter pollutant based on dates\n",
        "\n",
        "# convert to date format\n",
        "# date_main['Date'] = pd.to_datetime(date_main[['year', 'month', 'day']])\n",
        "# merge_pollutant_geo['Date'] = merge_pollutant_geo[\"Date Local\"]\n",
        "# print(merge_pollutant_geo.dtypes)\n",
        "# merge_pollutant_geo_date = pd.merge(merge_pollutant_geo, date_main[['Date']], on='Date', how='inner')\n",
        "# # print(merge_pollutant_geo_date.shape[0])\n",
        "# print(merge_pollutant_geo_date.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8SHv3Zxw4uB",
        "outputId": "5d5b28e3-b77d-411f-e26d-dabde3d19bb1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>Winter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Winter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>Winter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Winter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>Winter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys  day  month  year day_of_week  season\n",
              "0               1    1      1  1980     Tuesday  Winter\n",
              "1               2    2      1  1980   Wednesday  Winter\n",
              "2               3    3      1  1980    Thursday  Winter\n",
              "3               4    4      1  1980      Friday  Winter\n",
              "4               5    5      1  1980    Saturday  Winter"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "date_main.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzZme8Yyw4uC",
        "outputId": "bf54903f-10d3-4b81-ef52-dfc51294c46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "210\n",
            "   Surrogate Keys_x           ID      Lat       Lon  Status            City  \\\n",
            "0                 1  USC00042863  33.1211 -117.0900  Active       Escondido   \n",
            "1                 2  USC00166584  31.8142  -93.0856  Active    Natchitoches   \n",
            "2                 3  USC00280734  40.8292  -75.0833  Active       Belvidere   \n",
            "3                 4  USC00286055  40.4728  -74.4225  Active  East Brunswick   \n",
            "4                 5  USC00356749  45.5181 -122.6894  Active        Portland   \n",
            "\n",
            "        State  Surrogate Keys_y  Area Code  Population  Population Density  \\\n",
            "0  California               NaN        NaN         NaN                 NaN   \n",
            "1   Louisiana               NaN        NaN         NaN                 NaN   \n",
            "2  New Jersey               NaN        NaN         NaN                 NaN   \n",
            "3  New Jersey               NaN        NaN         NaN                 NaN   \n",
            "4      Oregon             242.0    38900.0   2072553.0              1881.0   \n",
            "\n",
            "              Timezone  \n",
            "0                  NaN  \n",
            "1                  NaN  \n",
            "2                  NaN  \n",
            "3                  NaN  \n",
            "4  America/Los_Angeles  \n",
            "210\n"
          ]
        }
      ],
      "source": [
        "# filter station by geo data\n",
        "print(station_df.shape[0])\n",
        "# print(geography_df.head())\n",
        "\n",
        "filtered_df_test = pd.merge(station_df, geography_df,left_on= ['City','State'] , right_on=['City',\"State\"], how='left')\n",
        "print(filtered_df_test.head())\n",
        "# filter the weather data by location and date\n",
        "# station_df[\"Scale Type\"] =\n",
        "print(filtered_df_test.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the assumption established in the report, the scaling type measured by the monitoring stations was agreed upon to be decided by the population density of the city in which the station is located in. Given the supporting references, the `National` scaling type is removed since there is no measure of the whole nation (or even global) recoreded or mentioned in the nature of the dataset.\n",
        "The default value of `Unknown` is given to those records where no location data was able to be extracted, hence these records will ultimately be dropped in the later steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaz7FV9aw4uC"
      },
      "outputs": [],
      "source": [
        "def get_station_type(density):\n",
        "    # print(density)\n",
        "\n",
        "    if(density > 0 and density <= 500):\n",
        "        return \"Micro\"\n",
        "    if(density > 501 and density <= 2000):\n",
        "        return \"Middle\"\n",
        "    if(density > 2001 and density <= 4000):\n",
        "        return \"Neighbourhood\"\n",
        "    if(density > 4001 and density <= 6500):\n",
        "        return \"Urban\"\n",
        "    if(density > 6501 and density <= 11000):\n",
        "        return \"Regional\"\n",
        "    return \"Unknown\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSTVad-rw4uC",
        "outputId": "7f95e4a2-23ce-427d-ca4a-254470b87d7c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>ID</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Lon</th>\n",
              "      <th>Status</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Scale Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>USC00042863</td>\n",
              "      <td>33.1211</td>\n",
              "      <td>-117.0900</td>\n",
              "      <td>Active</td>\n",
              "      <td>Escondido</td>\n",
              "      <td>California</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>USC00166584</td>\n",
              "      <td>31.8142</td>\n",
              "      <td>-93.0856</td>\n",
              "      <td>Active</td>\n",
              "      <td>Natchitoches</td>\n",
              "      <td>Louisiana</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>USC00280734</td>\n",
              "      <td>40.8292</td>\n",
              "      <td>-75.0833</td>\n",
              "      <td>Active</td>\n",
              "      <td>Belvidere</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>USC00286055</td>\n",
              "      <td>40.4728</td>\n",
              "      <td>-74.4225</td>\n",
              "      <td>Active</td>\n",
              "      <td>East Brunswick</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>USC00356749</td>\n",
              "      <td>45.5181</td>\n",
              "      <td>-122.6894</td>\n",
              "      <td>Active</td>\n",
              "      <td>Portland</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>Middle</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys           ID      Lat       Lon  Status            City  \\\n",
              "0               1  USC00042863  33.1211 -117.0900  Active       Escondido   \n",
              "1               2  USC00166584  31.8142  -93.0856  Active    Natchitoches   \n",
              "2               3  USC00280734  40.8292  -75.0833  Active       Belvidere   \n",
              "3               4  USC00286055  40.4728  -74.4225  Active  East Brunswick   \n",
              "4               5  USC00356749  45.5181 -122.6894  Active        Portland   \n",
              "\n",
              "        State Scale Type  \n",
              "0  California    Unknown  \n",
              "1   Louisiana    Unknown  \n",
              "2  New Jersey    Unknown  \n",
              "3  New Jersey    Unknown  \n",
              "4      Oregon     Middle  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "station_df[\"Scale Type\"] = filtered_df_test[\"Population Density\"].apply(get_station_type)\n",
        "station_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q0dMHivw4uD"
      },
      "source": [
        "A measure to provide maximum visibility ranges for users, denoting the range of sight possible correlated with the level of pollution in the area (able to view **up to** the provided value in miles). This is a new measure as part of the FACT table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3YMvlD0w4uL"
      },
      "outputs": [],
      "source": [
        "# 1: Hazardous (301-500)\n",
        "# 1.5: Very Unhealthy (201-300)\n",
        "# 3: Unhealthy (151-200)\n",
        "# 5: Unhealthy for Sensitive Groups (101-150)\n",
        "# 10: Moderate (51-100)\n",
        "# 20: Good (0-50)\n",
        "def visibilty_range(type):\n",
        "\n",
        "    if(type == \"Hazardous\"):\n",
        "        return 1\n",
        "    if(type == \"Very Unhealthy\"):\n",
        "        return 1.5\n",
        "    if(type == \"Unhealthy\"):\n",
        "        return 3\n",
        "    if(type == \"Unhealthy for Sensitive Groups\"):\n",
        "        return 5\n",
        "    if(type == \"Moderate\"):\n",
        "        return 10\n",
        "    if(type == \"Good\"):\n",
        "        return 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KcDR19Nw4uM",
        "outputId": "329d44b7-7a19-4815-a555-de4ce2295c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Geo Key  Date Key  Visibility Range\n",
            "0        1         1              20.0\n",
            "1        2         2              20.0\n",
            "2        3         3              20.0\n",
            "3        4         4              20.0\n",
            "4        5         5              20.0\n"
          ]
        }
      ],
      "source": [
        "# Creating the FACT table\n",
        "fact_df = pd.DataFrame()\n",
        "# Date key\n",
        "# Geo Key\n",
        "# Pollutant Key\n",
        "# weather Key\n",
        "# station Key\n",
        "\n",
        "fact_df['Geo Key'] = geography_df['Surrogate Keys']\n",
        "fact_df['Date Key'] = date_main['Surrogate Keys']\n",
        "fact_df[\"Visibility Range\"]  = usAQi_df['Category'].apply(visibilty_range)\n",
        "print(fact_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKJdlYJsw4uN",
        "outputId": "bb1209cd-407b-4565-b757-c1ecf3a43c55"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys_x</th>\n",
              "      <th>Date</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>prcp</th>\n",
              "      <th>station_id</th>\n",
              "      <th>Surrogate Keys_y</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1874-01-01</td>\n",
              "      <td>60.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USW00003822</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186066.0</td>\n",
              "      <td>1874-01-01</td>\n",
              "      <td>44.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>USW00093821</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>378978.0</td>\n",
              "      <td>1874-01-01</td>\n",
              "      <td>58.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USW00003017</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>433760.0</td>\n",
              "      <td>1874-01-01</td>\n",
              "      <td>40.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>USW00014768</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>585072.0</td>\n",
              "      <td>1874-01-01</td>\n",
              "      <td>41.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.09</td>\n",
              "      <td>USW00093820</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys_x       Date  tmax  tmin  prcp   station_id  \\\n",
              "0               1.0 1874-01-01  60.0  36.0  0.00  USW00003822   \n",
              "1          186066.0 1874-01-01  44.0  24.0  0.05  USW00093821   \n",
              "2          378978.0 1874-01-01  58.0  37.0  0.00  USW00003017   \n",
              "3          433760.0 1874-01-01  40.0  34.0  0.00  USW00014768   \n",
              "4          585072.0 1874-01-01  41.0  33.0  0.09  USW00093820   \n",
              "\n",
              "   Surrogate Keys_y  day  month  year day_of_week season  \n",
              "0               NaN  NaN    NaN   NaN         NaN    NaN  \n",
              "1               NaN  NaN    NaN   NaN         NaN    NaN  \n",
              "2               NaN  NaN    NaN   NaN         NaN    NaN  \n",
              "3               NaN  NaN    NaN   NaN         NaN    NaN  \n",
              "4               NaN  NaN    NaN   NaN         NaN    NaN  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Outer join\n",
        "date_main['Date'] = pd.to_datetime(date_main[['year', 'month', 'day']])\n",
        "\n",
        "weather_df_outerJoin_date= pd.merge(weather_df,date_main,on=\"Date\", how=\"outer\")\n",
        "weather_df_outerJoin_date.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3SJ2UdRw4uN"
      },
      "source": [
        "Using many inner join commands, all the filtered datasets are merged into one  `.csv` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIx_WHMSw4uN"
      },
      "outputs": [],
      "source": [
        "# date_weather_df = merging date and weather dfs\n",
        "# date_weather_pollutant_df = merging date_weather and pollutant dfs\n",
        "# geography_station_df = merging geo and station dfs\n",
        "\n",
        "# All together df = date_weather_pollutant and geography_station dfs\n",
        "date_main['Date'] = pd.to_datetime(date_main[['year', 'month', 'day']])\n",
        "\n",
        "date_weather_df = pd.merge(weather_df,date_main,on=\"Date\", how=\"inner\")\n",
        "date_weather_df_pollutant = pd.merge(date_weather_df, pollutant_df, on=\"Date\", how=\"inner\")\n",
        "print(date_weather_df_pollutant.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfFUXUbgw4uN",
        "outputId": "917f7a5f-f270-4ed5-cc34-386c705ce443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Surrogate Keys_x  Area Code         City       State  Population  \\\n",
            "0                 5      10580       Albany    New York      590823   \n",
            "1                 6      10740  Albuquerque  New Mexico      762853   \n",
            "2                 7      11100     Amarillo       Texas      202902   \n",
            "3                15      12020       Athens     Georgia      143081   \n",
            "4                17      12260      Augusta     Georgia      404125   \n",
            "\n",
            "   Population Density          Timezone  Surrogate Keys_y           ID  \\\n",
            "0              1747.0  America/New_York                83  USW00014735   \n",
            "1              1155.0    America/Denver               132  USW00023050   \n",
            "2               751.0   America/Chicago               131  USW00023047   \n",
            "3               414.0  America/New_York                52  USW00013873   \n",
            "4               252.0  America/New_York                13  USW00003820   \n",
            "\n",
            "       Lat       Lon  Status Scale Type  \n",
            "0  42.7431  -73.8092  Active     Middle  \n",
            "1  35.0419 -106.6156  Active     Middle  \n",
            "2  35.2333 -101.7089  Active     Middle  \n",
            "3  33.9481  -83.3275  Active      Micro  \n",
            "4  33.3644  -81.9633  Active      Micro  \n"
          ]
        }
      ],
      "source": [
        "geography_station_df = pd.merge(geography_df,station_df,on=[\"State\",\"City\"], how=\"inner\")\n",
        "print(geography_station_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each filtered dataset is saved into a `.csv` file, for easier processing by the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeIQSsdYw4uO"
      },
      "outputs": [],
      "source": [
        "# weather_df.to_csv(\"weather_df.csv\", index=False)\n",
        "# date_main.to_csv(\"date_main.csv\",index=False)\n",
        "# geography_df.to_csv(\"geography_df.csv\",index=False)\n",
        "# station_df.to_csv(\"station_df.csv\",index=False)\n",
        "# print(pollutant_df.head())\n",
        "# mode_value = pollutant_df['Category'].mode()[0]  # Get the mode of the 'Category' column\n",
        "# pollutant_df['Category'] = pollutant_df['Category'].fillna(mode_value)\n",
        "# print(pollutant_df.head())\n",
        "# pollutant_df = pollutant_df.reset_index()\n",
        "# pollutant_df = pollutant_df.drop(\"index\",axis=1)\n",
        "print(pollutant_df.head())\n",
        "pollutant_df.to_csv(\"pollutant_df.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New dataframes are created, based upon the previously transformed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgUh6rvow4uO"
      },
      "outputs": [],
      "source": [
        "csv_station = pd.read_csv('station_df.csv')\n",
        "csv_pollutant = pd.read_csv('pollutant_df.csv')\n",
        "csv_weather = pd.read_csv('weather_df.csv')\n",
        "csv_geography = pd.read_csv('geography_df.csv')\n",
        "csv_date = pd.read_csv('date_main.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now the merging commands begin; many different pairings of the datasets were merged previously (refer to the `Merging.ipynb` file for different trials). However, the results increased the number of rows by a vast number, not suitable for the final merging step (even not capable by the kernel ude to its limited hardware power). To avoid using hardware augmentation, other manners were sought out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WbmgO1_w4uP",
        "outputId": "d3c72bc7-56f1-4b97-893c-82eaa29fd5ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Surrogate Keys_x, Date, tmax, tmin, prcp, station_id, Surrogate Keys_y, ID, Lat, Lon, Status, City, State, Scale Type]\n",
            "Index: []\n",
            "   Surrogate Keys_x        Date  tmax  tmin  prcp   station_id  Status  \\\n",
            "0                 1  1874-01-01    60    36  0.00  USW00003822  Active   \n",
            "1                 2  1874-01-02    62    43  0.02  USW00003822  Active   \n",
            "2                 3  1874-01-03    66    53  0.12  USW00003822  Active   \n",
            "3                 4  1874-01-04    70    59  0.66  USW00003822  Active   \n",
            "4                 5  1874-01-05    66    58  0.50  USW00003822  Active   \n",
            "\n",
            "       City    State Scale Type  \n",
            "0  Savannah  Georgia     Middle  \n",
            "1  Savannah  Georgia     Middle  \n",
            "2  Savannah  Georgia     Middle  \n",
            "3  Savannah  Georgia     Middle  \n",
            "4  Savannah  Georgia     Middle  \n",
            "9757061\n"
          ]
        }
      ],
      "source": [
        "# weather and station dataframes based on the station id\n",
        "\n",
        "weather_station_df = pd.merge(csv_weather, csv_station, left_on=['station_id'], right_on=['ID'], how=\"inner\")\n",
        "print(weather_station_df.head(0))\n",
        "weather_station_df = weather_station_df.drop([\"Surrogate Keys_y\",\"ID\", 'Lat', 'Lon'],axis=1)\n",
        "print(weather_station_df.head())\n",
        "print(weather_station_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NeM6ehSw4uP",
        "outputId": "9e3aa429-0335-40ae-dc32-4215126ac3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Surrogate Keys, day, month, year, day_of_week, season, Date, Surrogate Keys_x, tmax, tmin, prcp, station_id, Status, City, State, Scale Type]\n",
            "Index: []\n",
            "   Surrogate Keys  day  month  year day_of_week  season        Date  tmax  \\\n",
            "0               1    1      1  1980     Tuesday  Winter  1980-01-01    50   \n",
            "1               1    1      1  1980     Tuesday  Winter  1980-01-01    58   \n",
            "2               1    1      1  1980     Tuesday  Winter  1980-01-01    47   \n",
            "3               1    1      1  1980     Tuesday  Winter  1980-01-01    28   \n",
            "4               1    1      1  1980     Tuesday  Winter  1980-01-01    35   \n",
            "\n",
            "   tmin  prcp   station_id  Status            City        State Scale Type  \n",
            "0    42   0.0  USW00003822  Active        Savannah      Georgia     Middle  \n",
            "1    52   0.0  USW00024286  Active   Crescent City   California     Middle  \n",
            "2    37   0.0  USW00013833  Active     Hattiesburg  Mississippi      Micro  \n",
            "3    22   0.0  USW00023066  Active  Grand Junction     Colorado     Middle  \n",
            "4    30   0.0  USW00093821  Active      Louisville     Kentucky    Unknown  \n",
            "3130359\n"
          ]
        }
      ],
      "source": [
        "# date and weather_station dataframes based on the date\n",
        "date_weather_station_df = pd.merge(csv_date, weather_station_df, on=\"Date\", how=\"inner\")\n",
        "print(date_weather_station_df.head(0))\n",
        "date_weather_station_df = date_weather_station_df.drop([\"Surrogate Keys_x\"],axis=1)\n",
        "print(date_weather_station_df.head())\n",
        "print(date_weather_station_df.shape[0])\n",
        "date_weather_station_df.to_csv('date_weather_station_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiaBgywSw4uP",
        "outputId": "dc510922-0f43-4a42-cc39-83631a317c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, Surrogate Keys_x, Date, State, City, NO2 Mean, CO Mean, SO2 Mean, O3 Mean, PM2.5 Mean, PM10 Mean, Category, Surrogate Keys_y, Area Code, Population, Population Density, Timezone]\n",
            "Index: []\n",
            "   Surrogate Keys_x        Date     State    City  NO2 Mean   CO Mean  \\\n",
            "0                 1  1980-01-01  Illinois  Peoria  0.000000  0.462500   \n",
            "1               688  1980-01-02  Illinois  Peoria  0.030000  0.858333   \n",
            "2              1651  1980-01-03  Illinois  Peoria  0.066957  0.770000   \n",
            "3              1897  1980-01-04  Illinois  Peoria  0.038636  1.266667   \n",
            "4              2420  1980-01-05  Illinois  Peoria  0.000000  1.137500   \n",
            "\n",
            "   SO2 Mean   O3 Mean  PM2.5 Mean  PM10 Mean Category  Area Code  Population  \\\n",
            "0  0.000375  0.008706    0.015688      0.024     Good      37900      253461   \n",
            "1  0.001208  0.002529    0.015688      0.024     Good      37900      253461   \n",
            "2  0.006987  0.004235    0.015688      0.024     Good      37900      253461   \n",
            "3  0.010042  0.003824    0.015688      0.024     Good      37900      253461   \n",
            "4  0.008063  0.008647    0.015688      0.024     Good      37900      253461   \n",
            "\n",
            "   Population Density         Timezone  \n",
            "0               906.0  America/Chicago  \n",
            "1               906.0  America/Chicago  \n",
            "2               906.0  America/Chicago  \n",
            "3               906.0  America/Chicago  \n",
            "4               906.0  America/Chicago  \n",
            "3930979\n"
          ]
        }
      ],
      "source": [
        "# geography and pollutant dataframes based on the city and state\n",
        "geography_pollutant_df = pd.merge( csv_pollutant, csv_geography, on=['State','City'], how=\"inner\")\n",
        "print(geography_pollutant_df.head(0))\n",
        "geography_pollutant_df = geography_pollutant_df.drop([\"Unnamed: 0\",\"Surrogate Keys_y\"],axis=1)\n",
        "print(geography_pollutant_df.head())\n",
        "print(geography_pollutant_df.shape[0])\n",
        "geography_pollutant_df.to_csv('geography_pollutant_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `final_df` dataframe as a result of the `Merging.ipynb` file, where the new measures are appended, based upon calculated attributes, and extracted attributes of the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaIcKefAw4uQ"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv(\"final_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y7A4BkUw4uR"
      },
      "outputs": [],
      "source": [
        "final_df.head()\n",
        "final_df[\"Visibility Range\"]  = final_df['Category'].apply(visibilty_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XSYGo2zw4uR",
        "outputId": "54c38a2d-31b6-461f-863f-c5ab23a3234d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>season</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Population</th>\n",
              "      <th>Population Density</th>\n",
              "      <th>...</th>\n",
              "      <th>Status</th>\n",
              "      <th>NO2 Mean</th>\n",
              "      <th>CO Mean</th>\n",
              "      <th>SO2 Mean</th>\n",
              "      <th>O3 Mean</th>\n",
              "      <th>PM2.5 Mean</th>\n",
              "      <th>PM10 Mean</th>\n",
              "      <th>Category</th>\n",
              "      <th>Visibility Range</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Active</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.206250</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Moderate</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1980-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Active</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>1.571667</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.011064</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Moderate</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1980-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Active</td>\n",
              "      <td>0.066957</td>\n",
              "      <td>1.549185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008588</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1980-01-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Active</td>\n",
              "      <td>0.038636</td>\n",
              "      <td>1.038095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007403</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1980-01-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Active</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.637500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006442</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1980-01-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys  day  month  year  season day_of_week    City  State  \\\n",
              "0               1    1      1  1980  Winter     Tuesday  Dallas  Texas   \n",
              "1               2    2      1  1980  Winter   Wednesday  Dallas  Texas   \n",
              "2               3    3      1  1980  Winter    Thursday  Dallas  Texas   \n",
              "3               4    4      1  1980  Winter      Friday  Dallas  Texas   \n",
              "4               5    5      1  1980  Winter    Saturday  Dallas  Texas   \n",
              "\n",
              "   Population  Population Density  ...  Status  NO2 Mean   CO Mean  SO2 Mean  \\\n",
              "0     5910669              1522.0  ...  Active  0.000000  1.206250  0.002083   \n",
              "1     5910669              1522.0  ...  Active  0.030000  1.571667  0.001500   \n",
              "2     5910669              1522.0  ...  Active  0.066957  1.549185  0.000000   \n",
              "3     5910669              1522.0  ...  Active  0.038636  1.038095  0.000000   \n",
              "4     5910669              1522.0  ...  Active  0.000000  0.637500  0.000000   \n",
              "\n",
              "    O3 Mean PM2.5 Mean PM10 Mean  Category  Visibility Range       Date  \n",
              "0  0.020117   0.015688     0.024  Moderate              10.0 1980-01-01  \n",
              "1  0.011064   0.015688     0.024  Moderate              10.0 1980-01-02  \n",
              "2  0.008588   0.015688     0.024      Good              20.0 1980-01-03  \n",
              "3  0.007403   0.015688     0.024      Good              20.0 1980-01-04  \n",
              "4  0.006442   0.015688     0.024      Good              20.0 1980-01-05  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df[['year', 'month', 'day']])\n",
        "final_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkgD3APpw4uR"
      },
      "outputs": [],
      "source": [
        "# The AQI values of the main dataset are merged with the final_df DataFrame based on the 'Date', 'State', and 'City' columns\n",
        "final_df_test = pd.merge(final_df,usAQi_df,left_on=['Date',\"State\",\"City\"], right_on=['Date',\"state_name\",\"city_ascii\"], how=\"inner\")\n",
        "final_df_test['AQI']\n",
        "# The unnecessary columns are dropped\n",
        "final_df = final_df_test.drop([\"Defining Parameter\",\"Number of Sites Reporting\",\"city_ascii\",\"state_id\",'state_name','lat','lng','population','density',\"timezone\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOQDNsNSw4uR",
        "outputId": "f628b821-9f75-41ab-8dd3-0fffd1c6baee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Surrogate Keys', 'day', 'month', 'year', 'season', 'day_of_week',\n",
            "       'City', 'State', 'Population', 'Population Density', 'Timezone',\n",
            "       'Area Code', 'tmax', 'tmin', 'prcp', 'ID', 'Scale Type', 'Status',\n",
            "       'NO2 Mean', 'CO Mean', 'SO2 Mean', 'O3 Mean', 'PM2.5 Mean', 'PM10 Mean',\n",
            "       'Category_x', 'Visibility Range', 'Date', 'Unnamed: 0', 'CBSA Code',\n",
            "       'AQI', 'Category_y'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(final_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhQztBZRw4uS"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop(['CBSA Code','Unnamed: 0',\"Category_y\"],axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method is for obtaining the AQHI (air quality health index) values based on the AQI (air quality index) values. AQI communicates the air quality of the single worst pollutant. The index rating for the new AQHI is the sum of the health risks from each of the pollutants in the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ISIFNaNw4uS"
      },
      "outputs": [],
      "source": [
        "def getAQHi(n):\n",
        "    if(n >= 0 and n <= 50):\n",
        "        return 0\n",
        "    if(n >= 51 and n <= 100):\n",
        "        return 1\n",
        "    if(n >= 101 and n <= 150):\n",
        "        return 2\n",
        "    else:\n",
        "        return 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2jEbTkGw4uS"
      },
      "outputs": [],
      "source": [
        "final_df[\"AQHI\"]  = final_df['AQI'].apply(getAQHi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFbIw_pZw4uS",
        "outputId": "cf8ef9f3-d9bf-443e-ba4b-d457a5615a94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surrogate Keys</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>season</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Population</th>\n",
              "      <th>Population Density</th>\n",
              "      <th>...</th>\n",
              "      <th>NO2 Mean</th>\n",
              "      <th>CO Mean</th>\n",
              "      <th>SO2 Mean</th>\n",
              "      <th>O3 Mean</th>\n",
              "      <th>PM2.5 Mean</th>\n",
              "      <th>PM10 Mean</th>\n",
              "      <th>Category</th>\n",
              "      <th>Visibility Range</th>\n",
              "      <th>AQI</th>\n",
              "      <th>AQHI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.206250</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Moderate</td>\n",
              "      <td>10.0</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>1.571667</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.011064</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Moderate</td>\n",
              "      <td>10.0</td>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066957</td>\n",
              "      <td>1.549185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008588</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038636</td>\n",
              "      <td>1.038095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007403</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1980</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Texas</td>\n",
              "      <td>5910669</td>\n",
              "      <td>1522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.637500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006442</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.024</td>\n",
              "      <td>Good</td>\n",
              "      <td>20.0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Surrogate Keys  day  month  year  season day_of_week    City  State  \\\n",
              "0               1    1      1  1980  Winter     Tuesday  Dallas  Texas   \n",
              "1               2    2      1  1980  Winter   Wednesday  Dallas  Texas   \n",
              "2               3    3      1  1980  Winter    Thursday  Dallas  Texas   \n",
              "3               4    4      1  1980  Winter      Friday  Dallas  Texas   \n",
              "4               5    5      1  1980  Winter    Saturday  Dallas  Texas   \n",
              "\n",
              "   Population  Population Density  ...  NO2 Mean   CO Mean  SO2 Mean  \\\n",
              "0     5910669              1522.0  ...  0.000000  1.206250  0.002083   \n",
              "1     5910669              1522.0  ...  0.030000  1.571667  0.001500   \n",
              "2     5910669              1522.0  ...  0.066957  1.549185  0.000000   \n",
              "3     5910669              1522.0  ...  0.038636  1.038095  0.000000   \n",
              "4     5910669              1522.0  ...  0.000000  0.637500  0.000000   \n",
              "\n",
              "    O3 Mean  PM2.5 Mean PM10 Mean  Category Visibility Range  AQI  AQHI  \n",
              "0  0.020117    0.015688     0.024  Moderate             10.0   51     1  \n",
              "1  0.011064    0.015688     0.024  Moderate             10.0   57     1  \n",
              "2  0.008588    0.015688     0.024      Good             20.0   36     0  \n",
              "3  0.007403    0.015688     0.024      Good             20.0   38     0  \n",
              "4  0.006442    0.015688     0.024      Good             20.0   38     0  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df = final_df.rename(columns={'Category_x':\"Category\"})\n",
        "final_df = final_df.drop(\"Date\",axis=1)\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7I-OFUDw4uS",
        "outputId": "6c565201-c186-4c62-c021-30dacb5e44d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Surrogate Keys', 'day', 'month', 'year', 'season', 'day_of_week',\n",
              "       'City', 'State', 'Population', 'Population Density', 'Timezone',\n",
              "       'Area Code', 'tmax', 'tmin', 'prcp', 'ID', 'Scale Type', 'Status',\n",
              "       'NO2 Mean', 'CO Mean', 'SO2 Mean', 'O3 Mean', 'PM2.5 Mean', 'PM10 Mean',\n",
              "       'Category', 'Visibility Range', 'AQI', 'AQHI'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmPoLtugw4uT"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv(\"Data_Staging.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final dataframe `final_df` is now converted into its `.csv` couterpart (`Data_Staging.csv`) to be used to load in the database instance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
